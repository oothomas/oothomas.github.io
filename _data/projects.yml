- title: "Trustworthy Exencephaly Detection with Focal‑Loss Transformers"
  description: |
    We build an imbalance‑aware, interpretable pipeline to detect exencephaly in late‑gestation
    mouse embryos from high‑throughput diceCT volumes. A self‑supervised transformer (modified M3T)
    is fine‑tuned under three regimes—cross‑entropy (CE‑Large), focal‑loss equal‑capacity (FL‑Large),
    and focal‑loss reduced‑capacity (FL‑Small). The training recipe combines class‑balanced mini‑batches,
    focal loss to preserve gradient on rare positives, and explicit capacity control to reduce overfitting.
    Interpretability is a first‑class goal: we compute Integrated Gradients, register saliency to a common
    atlas, and aggregate across five random seeds per regime (15 models total) to quantify explanation
    sparsity and reproducibility (saliency entropy; cross‑seed Dice/Jaccard).

    On 253 E15.5 embryos (24 with exencephaly; ~10% positives), all models achieved near‑perfect
    recognition (mean accuracy 0.996 ± 0.002; some seeds/regimens reached 1.000). Crucially, focal loss
    produced sparser, more stable explanations—reducing saliency entropy by up to 1.5 bits and roughly
    doubling cross‑seed Dice overlap—while concentrating attribution on the malformed cranial vault and
    suppressing spurious body‑wide signal. The focal‑loss + smaller‑capacity configuration preserved
    perfect sensitivity in affected embryos and improved attribution consistency without sacrificing accuracy.

    The workflow provides trustworthy, scalable screening for severe yet infrequent defects
    in large knockout cohorts. It relies on standard atlas registration and image pre‑processing, and requires no
    voxel‑level annotations, and generalizes naturally to other structural malformations and developmental
    stages—enabling reproducible triage, clearer downstream histology decisions, and cleaner signals for
    follow‑up morphometrics or genotype–phenotype studies.
  # Optional quick-link buttons (show as buttons if present)
  paper_url: "https://www.biorxiv.org/content/10.1101/2025.08.12.669840v1"
  repo_url:  "https://github.com/oothomas/M3T_Classifer"
  demo_url:  "https://github.com/oothomas/M3T_Classifer"   # optional

  # Optional full citation string (shown under buttons)
  publication: "Thomas, O. O., Roston, R., Shen, H., & Maga, A. M. (2025). Trustworthy detection of exencephaly in high‑throughput micro‑CT embryo screens with focal‑loss transformers. *bioRxiv*. https://doi.org/10.1101/2025.08.12.669840"

  images:
    - thumb: "assets/images/projects/exencephaly-pipeline-thumb.png"
      full:  "assets/images/projects/exencephaly-pipeline-full.png"
      caption: "Pipeline schematic: class‑balanced sampling, focal‑loss fine‑tuning, capacity control, and seed‑ensemble Integrated Gradients registered to an atlas for stable, localized explanations."
      alt: "Functional-map pipeline diagram"  # optional
    - thumb: "assets/images/projects/exencephaly-saliency-thumb.png"
      full:  "assets/images/projects/exencephaly-saliency-full.png"
      caption: "Ensemble‑averaged attribution maps highlight the malformed cranial vault in positive and negative cases and suppress off‑target signal; focal loss lowers saliency entropy and boosts cross‑seed Dice/Jaccard."


- title: "SlicerMorph Photogrammetry: SAM‑NodeODM Pipeline for High‑Fidelity 3D Specimens"
  description: |
    This project delivers a fully open, end‑to‑end photogrammetry workflow inside 3D Slicer that
    integrates automatic image masking via the Segment Anything Model (SAM) with surface reconstruction
    using OpenDroneMap’s NodeODM, wrapped in a user‑friendly Slicer extension. The module unifies image
    import, batch/single‑image masking, optional scaling via ArUco/GCPs, reconstruction, and direct model
    import for morphometric analysis.

    Goals were twofold: streamline background removal and improve geometric accuracy, especially around
    delicate cranial structures. Using 14 mountain‑beaver skulls with micro‑CT as reference, the updated
    workflow lowered mean surface distance and RMSE by ~10–15% across specimens versus our prior open‑source
    pipeline, while Hausdorff changed little—evidence that gains were global rather than driven by a few
    outliers. Qualitatively, thin elements (e.g., zygomatic arches, orbital rims) showed fewer breaks,
    clogs, and smoothing artifacts. A Taguchi L16 design efficiently explored NodeODM parameters
    (e.g., mesh‑octree‑depth, mesh‑size, feature/pc quality), yielding a reproducible configuration that
    balances fidelity and runtime; all job configs are saved as JSON for auditability.

    Findings: (1) Automated SAM masking markedly reduces manual cleanup and operator bias;
    (2) tuned NodeODM reconstruction produces smoother, more anatomically faithful meshes; and
    (3) the one‑ecosystem approach (mask → reconstruct → analyze) accelerates research hand‑offs to
    SlicerMorph morphometrics. Broader impact spans natural‑history collections, ecology/evolution studies,
    teaching/outreach, and large‑cohort digitization where cost, licensing, and throughput matter. Cloud‑ready
    deployment (MorphoCloud) supports GPU‑accelerated processing without local DevOps overhead.
  # Optional quick-link buttons (show as buttons if present)
  paper_url: "https://doi.org/10.1242/bio.062126"
  repo_url:  "https://github.com/SlicerMorph/SlicerPhotogrammetry"
  demo_url:  "https://github.com/SlicerMorph/SlicerPhotogrammetry"   # optional

  # Optional full citation string (shown under buttons)
  publication: "Thomas, O. O., Zhang, C., & Maga, A. M. (2025). SlicerMorph photogrammetry: An open‑source photogrammetry workflow for reconstructing 3D models. *Biology Open, 14*, bio062126. https://doi.org/10.1242/bio.062126"

  images:
    - thumb: "assets/images/projects/slicerphotogrammetry-ui-thumb.png"
      full:  "assets/images/projects/slicerphotogrammetry-ui-full.png"
      caption: "Workflow & UI: SAM-based batch masking and NodeODM reconstruction inside 3D Slicer, with the reconstructed textured skull imported directly for analysis."
      alt: "Functional-map pipeline diagram"  # optional
    - thumb: "assets/images/projects/slicerphotogrammetry-compare-thumb.png"
      full:  "assets/images/projects/slicerphotogrammetry-compare-full.png"
      caption: "Side‑by‑side reconstructions with deviation heatmaps versus micro‑CT; the updated pipeline reduces artifacts around thin structures (e.g., zygomatic arches) and lowers mean/RMSE errors."

- title: "Spatially & Spectrally Consistent morphVQ: Functional‑Map Landmarking for Mouse Mandibles"
  description: |
    This project develops an automated pipeline for anatomical landmark placement on 3D mouse
    hemi‑mandibles using descriptor learning and the functional map (FMap) framework. Instead of
    supervising to landmarks, the model learns intrinsic shape descriptors and dense functional
    correspondences in an unsupervised fashion, then recovers point‑to‑point maps to transfer
    landmarks across specimens. The workflow removes the need for rigid pre‑alignment and targets
    speed, accuracy, and generalizability.

    Methodologically, three upgrades over prior morphVQ are key: (1) orientation‑preserving Complex
    FMaps to avoid symmetry flips and eliminate pre‑alignment; (2) DiffusionNet for robust per‑vertex
    descriptors across mesh resolutions and surface complexities; and (3) spatial + spectral cycle
    consistency during Deep Functional Maps (DFMaps) training to enforce near‑bijective mappings and
    reduce post‑hoc refinement. Training is pairwise on meshes (LBO basis, WKS inputs), and dense
    correspondences are converted to landmark estimates via nearest‑neighbor transfer, aggregated with
    median selection for stability.

    Findings on a 425‑specimen mandible dataset show that the approach is faster than MALPACA while
    maintaining competitive accuracy. Although MALPACA often yields the lowest RMSE, AutoLandmark‑FM
    performs comparably—especially with smaller training sets—indicating strong generalization and
    practical viability. Visual checks confirm that automated placements typically fall within the
    acceptable range observed for MALPACA estimates.

    By combining unsupervised descriptor learning with functional correspondences, the
    pipeline scales landmarking to large cohorts, reduces manual effort, and flexibly adapts as
    hypotheses change (no re‑digitizing required). The result is a reproducible, high‑throughput
    alternative for morphometrics that plays nicely with open‑source tooling (e.g., 3D Slicer/SlicerMorph)
    and downstream statistical or ML analyses.

  # Optional quick-link buttons (show as buttons if present)
  paper_url: "https://onlinelibrary.wiley.com/doi/10.1111/joa.14196"
  repo_url:  "https://github.com/oothomas/SSC-MorphVQ"
  demo_url:  "https://oothomas.github.io/assets/docs/projects/interactive_morphospace.html"   # optional

  # Optional full citation string (shown under buttons)
  publication: "Thomas, O. O., & Maga, A. M. (2025). Leveraging descriptor learning and functional map‑based shape matching for automated anatomical landmarking in mouse mandibles. *Journal of Anatomy, 246*, 829–845. https://doi.org/10.1111/joa.14196"

  images:
    - thumb: "assets/images/projects/autolandmarkfm-arch-thumb.png"
      full:  "assets/images/projects/autolandmarkfm-arch-full.png"
      caption: "Network and training scheme: DiffusionNet descriptor learning, main and complex functional map branches, and cycle‑consistency enforcing near‑bijective correspondences for landmark transfer."
      alt: "Functional-map pipeline diagram"  # optional
    - thumb: "assets/images/projects/autolandmarkfm-landmarks-thumb.png"
      full:  "assets/images/projects/autolandmarkfm-landmarks-full.png"
      caption: "FMAP's estimates (green) closely align with both MALPACA's (red) and the ground truth (blue), with deviations within expected human error. The model effectively estimates landmarks on 3D anatomical structures, advancing morphometric analysis by integrating geometry processing and deep-learning techniques."

- title: "morphVQ: Functional-Map Morphometrics for Automated Phenotyping"
  description: |
    Most 3D shape analysis still relies on experts clicking landmarks—slow, biased, and blind to much of the surface. morphVQ replaces that with an automated 
    pipeline that learns dense correspondences across whole meshes and summarizes shape as surface‑wide expansions, 
    contractions, and angle changes—compact, interpretable descriptors.
    
    It scales unbiased quantification to large cohorts. On primate bones, morphVQ matched landmark 
    pipelines while capturing more detail and running faster. It also projects “where the differences are” back onto the surface for review.

    Beyond comparative anatomy, teams in medical imaging, preclinical phenotyping, surgical planning, and device design 
    can trade manual clicks for reproducible, full‑surface metrics. For ML teams, it showcases geometric deep learning for 
    correspondence learning and interpretable features.

  # Optional quick-link buttons (show as buttons if present)
  paper_url: "https://doi.org/10.1371/journal.pcbi.1009061"
  repo_url:  "https://github.com/oothomas/morphVQ"
  demo_url:  "https://oothomas.github.io/assets/docs/projects/correspondence_and_landmarks.html"   # optional

  # Optional full citation string (shown under buttons)
  publication: "Thomas, O. O., Shen, H., Raaum, R. L., Harcourt-Smith, W. E. H., Polk, J. D., & Hasegawa-Johnson, M. (2023). Automated morphological phenotyping using learned shape descriptors and functional maps: A novel approach to geometric morphometrics. *PLOS Computational Biology, 19*(1), e1009061. https://doi.org/10.1371/journal.pcbi.1009061"

  images:
    - thumb: "assets/images/projects/morphvq-pipeline-thumb.png"
      full:  "assets/images/projects/morphvq-pipeline-full.png"
      caption: "Pipeline overview: manual GM vs. auto3DGM vs. morphVQ with learned descriptors, functional maps, Consistent ZoomOut, and LSSDs (area‑based & conformal)."
      alt: "Functional-map pipeline diagram"  # optional
    - thumb: "assets/images/projects/morphvq-distinctiveness-thumb.png"
      full:  "assets/images/projects/morphvq-distinctiveness-full.png"
      caption: "LSSD‑derived distinctiveness maps highlighting surface regions that most separate biological groups; warmer colors indicate stronger discriminative signal."
